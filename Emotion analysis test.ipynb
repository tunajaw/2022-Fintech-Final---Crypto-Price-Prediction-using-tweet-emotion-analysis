{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\John8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\John8\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "import re\n",
    "import emojis\n",
    "import csv\n",
    "import collections\n",
    "import json\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "from textblob import TextBlob, Word\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "seg = Segmenter(corpus='twitter')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emotion_dicts.txt', 'r') as f:\n",
    "    emotion_dict = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the emotion corpus\n",
    "anger_dict = emotion_dict['anger']\n",
    "anticipation_dict = emotion_dict['anticipation']\n",
    "disgust_dict = emotion_dict['disgust']\n",
    "fear_dict = emotion_dict['fear']\n",
    "joy_dict = emotion_dict['joy']\n",
    "sadness_dict = emotion_dict['sadness']\n",
    "surprise_dict = emotion_dict['surprise']\n",
    "trust_dict = emotion_dict['trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the text in the hashtag\n",
    "def extract_hashtag_text(tweet):\n",
    "    tag_text = \"\"\n",
    "    tweet= re.findall(r'#(\\w+)', tweet)\n",
    "    for tag in tweet:\n",
    "        clean_tag=seg.segment(tag)\n",
    "        tag_text += (clean_tag + \" \")\n",
    "    return tag_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract text information in emoji\n",
    "def extract_emoji_text(tweet):\n",
    "    emoji_text = \"\"\n",
    "    emoji_list = emojis.get(tweet)\n",
    "    for emoji in emoji_list:\n",
    "        emoji = emojis.decode(emoji)\n",
    "        emoji = re.sub(r':', '', emoji) \n",
    "        emoji = re.sub(r'_', ' ', emoji) \n",
    "        emoji_text += (emoji + \" \")\n",
    "    return emoji_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up irrelevant symbols in tweets\n",
    "def tweets_cleaner(tweet):\n",
    "    #Use preprocessor to clean up URLs, Hashtags and user mentions\n",
    "    tweet = p.clean(tweet)\n",
    "    #Clean up all numbers\n",
    "    tweet = re.sub(r'[0-9]*', '', tweet) \n",
    "    #Clean up all punctuation\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet) \n",
    "    #Lowercase letters\n",
    "    semiclean_tweet = tweet.lower()\n",
    "    return semiclean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "def lemmatization_without_stopwords(semiclean_tweet):\n",
    "    lemmatized_list=[]\n",
    "    #Use TextBlob to tokenize tweets\n",
    "    sent = TextBlob(semiclean_tweet)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    #Realize lemmatization according to the corresponding POS\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
    "    #Remove stopwords\n",
    "    for wd, tag in words_and_tags:\n",
    "        if wd  not in stopwords:\n",
    "            lemmatized_list.append(wd.lemmatize(tag))\n",
    "    return lemmatized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get clean tweet word lists\n",
    "def get_clean_text(raw_daily_data):\n",
    "    \n",
    "    clean_text=[0]*len(raw_daily_data[\"Tweet\"])\n",
    "    \n",
    "    for num, tweet in enumerate(raw_daily_data[\"Tweet\"]):\n",
    "        t_ls = lemmatization_without_stopwords(tweets_cleaner(tweet))\n",
    "        e_ls = lemmatization_without_stopwords(extract_emoji_text(tweet))\n",
    "        h_ls = lemmatization_without_stopwords(extract_hashtag_text(tweet))\n",
    "        clean_text[num] = t_ls + e_ls + h_ls\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the emotion score of each tweet\n",
    "def get_emotion_score(dataframe,clean_text):\n",
    "    \n",
    "    anger_score = [0]*len(clean_text)\n",
    "    for i, clean in enumerate(clean_text):\n",
    "        counts = collections.Counter(clean)\n",
    "        for word, freq in counts.items():\n",
    "            anger_score[i] += anger_dict.get(word, 0) * freq\n",
    "\n",
    "    anticipation_score = [0]*len(clean_text)\n",
    "    for i, clean in enumerate(clean_text):\n",
    "        counts = collections.Counter(clean)\n",
    "        for word, freq in counts.items():\n",
    "            anticipation_score[i] += anticipation_dict.get(word, 0) * freq\n",
    "            \n",
    "    disgust_score = [0]*len(clean_text)\n",
    "    for i, clean in enumerate(clean_text):\n",
    "        counts = collections.Counter(clean)\n",
    "        for word, freq in counts.items():\n",
    "            disgust_score[i] += disgust_dict.get(word, 0) * freq\n",
    "    \n",
    "    fear_score = [0]*len(clean_text)\n",
    "    for i, clean in enumerate(clean_text):\n",
    "        counts = collections.Counter(clean)\n",
    "        for word, freq in counts.items():\n",
    "            fear_score[i] += fear_dict.get(word, 0) * freq\n",
    "    \n",
    "    joy_score = [0]*len(clean_text)\n",
    "    for i, clean in enumerate(clean_text):\n",
    "        counts = collections.Counter(clean)\n",
    "        for word, freq in counts.items():\n",
    "            joy_score[i] += joy_dict.get(word, 0) * freq\n",
    "    \n",
    "    sadness_score = [0]*len(clean_text)\n",
    "    for i, clean in enumerate(clean_text):\n",
    "        counts = collections.Counter(clean)\n",
    "        for word, freq in counts.items():\n",
    "            sadness_score[i] += sadness_dict.get(word, 0) * freq\n",
    "            \n",
    "    surprise_score = [0]*len(clean_text)\n",
    "    for i, clean in enumerate(clean_text):\n",
    "        counts = collections.Counter(clean)\n",
    "        for word, freq in counts.items():\n",
    "            surprise_score[i] += surprise_dict.get(word, 0) * freq\n",
    "    \n",
    "    trust_score = [0]*len(clean_text)\n",
    "    for i, clean in enumerate(clean_text):\n",
    "        counts = collections.Counter(clean)\n",
    "        for word, freq in counts.items():\n",
    "            trust_score[i] += trust_dict.get(word, 0) * freq\n",
    "            \n",
    "    dataframe['anger_score']=anger_score  \n",
    "    dataframe['anticipation_score']=anticipation_score\n",
    "    dataframe['disgust_score']=disgust_score\n",
    "    dataframe['fear_score']=fear_score\n",
    "    dataframe['joy_score']=joy_score\n",
    "    dataframe['sadness_score']=sadness_score\n",
    "    dataframe['surprise_score']=surprise_score\n",
    "    dataframe['trust_score']=trust_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the emotion score of the day\n",
    "def get_daily_emotion(emotion_df):\n",
    "    d_anger_score=0\n",
    "    d_anticipation_score=0\n",
    "    d_disgust_score=0\n",
    "    d_fear_score=0\n",
    "    d_joy_score=0\n",
    "    d_sadness_score=0\n",
    "    d_surprise_score=0\n",
    "    d_trust_score=0\n",
    "    \n",
    "    for i in range(0,len(emotion_df)):\n",
    "        d_anger_score += (1+0.01*emotion_df.iloc[i]['like'])*emotion_df.iloc[i]['anger_score']\n",
    "        #Use like as an additional weight for tweets, and each like represents 1% of the sentiment score\n",
    "        d_anticipation_score += (1+0.01*emotion_df.iloc[i]['like'])*emotion_df.iloc[i]['anticipation_score']\n",
    "        d_disgust_score +=(1+0.01*emotion_df.iloc[i]['like'])*emotion_df.iloc[i]['disgust_score']\n",
    "        d_fear_score +=(1+0.01*emotion_df.iloc[i]['like'])*emotion_df.iloc[i]['fear_score']\n",
    "        d_joy_score +=(1+0.01*emotion_df.iloc[i]['like'])*emotion_df.iloc[i]['joy_score']\n",
    "        d_sadness_score +=(1+0.01*emotion_df.iloc[i]['like'])*emotion_df.iloc[i]['sadness_score']\n",
    "        d_surprise_score +=(1+0.01*emotion_df.iloc[i]['like'])*emotion_df.iloc[i]['surprise_score']\n",
    "        d_trust_score +=(1+0.01*emotion_df.iloc[i]['like'])*emotion_df.iloc[i]['trust_score']\n",
    "    \n",
    "    return d_anger_score,d_anticipation_score,d_disgust_score,d_fear_score,d_joy_score,d_sadness_score,d_surprise_score,d_trust_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get valid tweet data\n",
    "def get_available_emotion(df):\n",
    "    available_daily_data = df[ (df['anger_score'] != 0) | (df['anticipation_score']!=0) | (df['disgust_score']!=0) | (df['fear_score']!=0)| (df['joy_score']!=0) | (df['sadness_score']!=0) | (df['surprise_score']!=0) | (df['trust_score'] != 0)]\n",
    "    return available_daily_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "month= '1'\n",
    "coin='Bitcoin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_path = 'E:\\\\project_data\\\\tweet_data\\\\#'+coin+'\\\\'+ month\n",
    "\n",
    "filenames = glob.glob(data_input_path + '\\\\*.csv') \n",
    "\n",
    "daily_emotion=pd.DataFrame(columns=('date','volume','available_volume','anger_score','anticipation_score','disgust_score','fear_score','joy_score','sadness_score','surprise_score','trust_score'))\n",
    "\n",
    "for filename in filenames:\n",
    "    daily_data = pd.read_csv(filename, index_col = None, header = 0)\n",
    "    \n",
    "    #Get date\n",
    "    date = daily_data['created_at'][0][0:10]\n",
    "    \n",
    "    clean_text = get_clean_text(daily_data)\n",
    "    \n",
    "    #Get sentiment score for each tweet of the day\n",
    "    get_emotion_score(daily_data ,clean_text)\n",
    "    \n",
    "    #Get the total sentiment score of the day\n",
    "    anger_score,anticipation_score,disgust_score,fear_score,joy_score,sadness_score,surprise_score,trust_score = get_daily_emotion(daily_data)\n",
    "    \n",
    "    #Tweets volume\n",
    "    volume=len(daily_data)\n",
    "    \n",
    "    #Available tweets volume (including specific emotions)\n",
    "    available_volume=len(get_available_emotion(daily_data))\n",
    "    \n",
    "    #Write daily data to file\n",
    "    daily_emotion= daily_emotion.append({'date' : date , 'volume' :volume , 'available_volume': available_volume,\n",
    "                                         'anger_score' : anger_score , 'anticipation_score' : anticipation_score ,\n",
    "                                         'disgust_score': disgust_score , 'fear_score' : fear_score ,\n",
    "                                         'joy_score' : joy_score , 'sadness_score': sadness_score,\n",
    "                                         'surprise_score' : surprise_score , 'trust_score': trust_score}, \n",
    "                                         ignore_index = True)\n",
    "    print(filename)\n",
    "\n",
    "#Output the emotion report of the current month\n",
    "daily_emotion.to_csv('E:\\\\project_data\\\\tweet_data\\\\#'+coin+'\\\\report\\\\'+month+\"Report.csv\",encoding='utf-8',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "daliy_data = pd.read_csv('tweets.csv', index_col=0)\n",
    "clean_text = get_clean_text(daliy_data)\n",
    "get_emotion_score(daliy_data ,clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>like</th>\n",
       "      <th>retweet</th>\n",
       "      <th>reply</th>\n",
       "      <th>anger_score</th>\n",
       "      <th>anticipation_score</th>\n",
       "      <th>disgust_score</th>\n",
       "      <th>fear_score</th>\n",
       "      <th>joy_score</th>\n",
       "      <th>sadness_score</th>\n",
       "      <th>surprise_score</th>\n",
       "      <th>trust_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-30 23:59:58+00:00</td>\n",
       "      <td>@DashDobrofsky Soon all those people who belie...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.762291</td>\n",
       "      <td>3.919842</td>\n",
       "      <td>0.975390</td>\n",
       "      <td>4.050166</td>\n",
       "      <td>1.407615</td>\n",
       "      <td>0.575600</td>\n",
       "      <td>2.864855</td>\n",
       "      <td>2.895408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-30 23:59:56+00:00</td>\n",
       "      <td>@marlon_humphrey How's the Bitcoin going for y...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150243</td>\n",
       "      <td>1.211380</td>\n",
       "      <td>0.081535</td>\n",
       "      <td>0.039826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-30 23:59:51+00:00</td>\n",
       "      <td>https://t.co/cOIk7u7kY3\\nUS CFTC commissioner ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.587202</td>\n",
       "      <td>1.407864</td>\n",
       "      <td>0.475431</td>\n",
       "      <td>0.608304</td>\n",
       "      <td>0.289417</td>\n",
       "      <td>0.655437</td>\n",
       "      <td>1.860658</td>\n",
       "      <td>0.713978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-30 23:59:47+00:00</td>\n",
       "      <td>Bitcoin consumes energy on the scale of entire...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.093080</td>\n",
       "      <td>2.375762</td>\n",
       "      <td>0.219344</td>\n",
       "      <td>0.866195</td>\n",
       "      <td>2.786284</td>\n",
       "      <td>0.827080</td>\n",
       "      <td>0.439704</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-30 23:59:47+00:00</td>\n",
       "      <td>@Todd_Toddleston @Arcanineties I didn't say th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.449523</td>\n",
       "      <td>2.246428</td>\n",
       "      <td>1.596995</td>\n",
       "      <td>1.728888</td>\n",
       "      <td>0.145413</td>\n",
       "      <td>0.442848</td>\n",
       "      <td>0.900368</td>\n",
       "      <td>0.374857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2022-11-30 18:46:45+00:00</td>\n",
       "      <td>#Ethereum price update: \\n\\n#ETH $1272.21 USD\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484547</td>\n",
       "      <td>1.404695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2022-11-30 18:46:45+00:00</td>\n",
       "      <td>@ragz2crypto As I remember when Bitcoin want t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.835912</td>\n",
       "      <td>0.099790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>0.193291</td>\n",
       "      <td>0.450845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2022-11-30 18:46:38+00:00</td>\n",
       "      <td>Bitcoin and Ethereum show signs of a resurgenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300839</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2022-11-30 18:46:37+00:00</td>\n",
       "      <td>@tbrandall33 @LightHarmonious @WatcherGuru Bit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.209599</td>\n",
       "      <td>0.650404</td>\n",
       "      <td>0.169452</td>\n",
       "      <td>0.569501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2022-11-30 18:46:35+00:00</td>\n",
       "      <td>@TheCryptoLark Brazil has just officially reco...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380491</td>\n",
       "      <td>0.616990</td>\n",
       "      <td>0.455062</td>\n",
       "      <td>0.463840</td>\n",
       "      <td>0.407966</td>\n",
       "      <td>0.754245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Date  \\\n",
       "0     2022-11-30 23:59:58+00:00   \n",
       "1     2022-11-30 23:59:56+00:00   \n",
       "2     2022-11-30 23:59:51+00:00   \n",
       "3     2022-11-30 23:59:47+00:00   \n",
       "4     2022-11-30 23:59:47+00:00   \n",
       "...                         ...   \n",
       "9995  2022-11-30 18:46:45+00:00   \n",
       "9996  2022-11-30 18:46:45+00:00   \n",
       "9997  2022-11-30 18:46:38+00:00   \n",
       "9998  2022-11-30 18:46:37+00:00   \n",
       "9999  2022-11-30 18:46:35+00:00   \n",
       "\n",
       "                                                  Tweet  like  retweet  reply  \\\n",
       "0     @DashDobrofsky Soon all those people who belie...     0        0      0   \n",
       "1     @marlon_humphrey How's the Bitcoin going for y...     2        0      0   \n",
       "2     https://t.co/cOIk7u7kY3\\nUS CFTC commissioner ...     6        0      0   \n",
       "3     Bitcoin consumes energy on the scale of entire...     0        0      0   \n",
       "4     @Todd_Toddleston @Arcanineties I didn't say th...     0        0      1   \n",
       "...                                                 ...   ...      ...    ...   \n",
       "9995  #Ethereum price update: \\n\\n#ETH $1272.21 USD\\...     1        0      0   \n",
       "9996  @ragz2crypto As I remember when Bitcoin want t...     2        0      2   \n",
       "9997  Bitcoin and Ethereum show signs of a resurgenc...     0        0      0   \n",
       "9998  @tbrandall33 @LightHarmonious @WatcherGuru Bit...     0        0      1   \n",
       "9999  @TheCryptoLark Brazil has just officially reco...     0        0      0   \n",
       "\n",
       "      anger_score  anticipation_score  disgust_score  fear_score  joy_score  \\\n",
       "0        1.762291            3.919842       0.975390    4.050166   1.407615   \n",
       "1        0.150243            1.211380       0.081535    0.039826   0.000000   \n",
       "2        0.587202            1.407864       0.475431    0.608304   0.289417   \n",
       "3        1.093080            2.375762       0.219344    0.866195   2.786284   \n",
       "4        1.449523            2.246428       1.596995    1.728888   0.145413   \n",
       "...           ...                 ...            ...         ...        ...   \n",
       "9995     1.484547            1.404695       0.000000    0.000000   0.000000   \n",
       "9996     0.538475            0.000000       0.835912    0.099790   0.000000   \n",
       "9997     0.000000            0.667242       0.000000    0.000000   0.000000   \n",
       "9998     0.209599            0.650404       0.169452    0.569501   0.000000   \n",
       "9999     0.380491            0.616990       0.455062    0.463840   0.407966   \n",
       "\n",
       "      sadness_score  surprise_score  trust_score  \n",
       "0          0.575600        2.864855     2.895408  \n",
       "1          0.000000        0.000000     0.312855  \n",
       "2          0.655437        1.860658     0.713978  \n",
       "3          0.827080        0.439704     0.000000  \n",
       "4          0.442848        0.900368     0.374857  \n",
       "...             ...             ...          ...  \n",
       "9995       0.249774        0.000000     0.083404  \n",
       "9996       0.408701        0.193291     0.450845  \n",
       "9997       0.000000        0.300839     0.000000  \n",
       "9998       0.664927        0.000000     0.000000  \n",
       "9999       0.754245        0.000000     0.008534  \n",
       "\n",
       "[10000 rows x 13 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daliy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9848"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_available_emotion(daliy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_score,anticipation_score,disgust_score,fear_score,joy_score,sadness_score,surprise_score,trust_score = get_daily_emotion(daliy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9551.758661287771,\n",
       " 13657.278563124115,\n",
       " 6588.80182098704,\n",
       " 8139.579406207774,\n",
       " 7635.810692083374,\n",
       " 4326.253232588365,\n",
       " 7658.73602749644,\n",
       " 8695.228075828894)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anger_score,anticipation_score,disgust_score,fear_score,joy_score,sadness_score,surprise_score,trust_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "eba57746c2070bb69413183e0ca6d8717682b251202776e3354c8d9d4b177e4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
